{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过修改 Mocap 来控制 Franka Panda 机器人\n",
    "\n",
    "* 环境\n",
    "\n",
    "| Package   | Version |\n",
    "|-----------|---------|\n",
    "| gymnasium | 0.29.1  |\n",
    "| numpy | 1.24.0 |\n",
    "| scipy | 1.13.1 |\n",
    "| torch |  2.3.1+cu121 |\n",
    "| grpcio |   1.64.1 |\n",
    "| 关卡名 | Panda_Mocap |\n",
    "\n",
    "* Mocap 与机器人手部 ee_center_site 焊接\n",
    "* 根据强化学习模型输出的 Mocap xpos 修改 ee_site 的 xpos\n",
    "* 根据逆向动力学，手部关节跟随移动\n",
    "\n",
    "* 首先使用 reach 任务训练模型，确保机器人学会移动到目标附近\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_file_path = os.path.abspath('')\n",
    "project_root = os.path.dirname(current_file_path)\n",
    "\n",
    "# 将项目根目录添加到 PYTHONPATH\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from gymnasium.envs.registration import register\n",
    "from envs.mujoco.franka_emika_panda import FrankaEnv\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "from envs.orca_gym_env import ActionSpaceType\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def register_env(grpc_address, ):\n",
    "    print(\"register_env: \", grpc_address)\n",
    "    gym.register(\n",
    "        id=f\"PandaMocap-v0-OrcaGym-{grpc_address[-2:]}\",\n",
    "        entry_point=\"envs.panda_mocap.reach:FrankaReachEnv\",\n",
    "        kwargs={'frame_skip': 5, \n",
    "                'reward_type': \"sparse\",\n",
    "                'action_space_type': ActionSpaceType.CONTINUOUS,\n",
    "                'action_step_count': 0,\n",
    "                'grpc_address': grpc_address, \n",
    "                'agent_names': ['Panda'], \n",
    "                'time_step': 0.01},\n",
    "        max_episode_steps=512,\n",
    "        reward_threshold=0.0,\n",
    "    )\n",
    "\n",
    "async def continue_training(env, total_timesteps, is_training):\n",
    "\n",
    "    # 加载已有模型或初始化新模型\n",
    "    if os.path.exists(\"panda_mocap_ppo_model.zip\"):\n",
    "        model = PPO.load(\"panda_mocap_ppo_model\", env=env)\n",
    "    else:\n",
    "        # 定义自定义策略网络\n",
    "        policy_kwargs = dict(\n",
    "            net_arch=dict(\n",
    "                pi=[128, 128, 128],  # 策略网络结构\n",
    "                vf=[128, 128, 128]   # 值函数网络结构\n",
    "            ),\n",
    "            ortho_init=True,\n",
    "            activation_fn=nn.ReLU\n",
    "        )\n",
    "        model = PPO(\"MultiInputPolicy\", env, verbose=1, learning_rate=0.0003, n_steps=2048, batch_size=128, gamma=0.95, clip_range=0.2, policy_kwargs=policy_kwargs)\n",
    "        \n",
    "\n",
    "    # 训练模型，每 LOOP_LEN 步保存一次模型\n",
    "    if (is_training):\n",
    "        LOOP_LEN = 100000\n",
    "        if (total_timesteps >= LOOP_LEN):\n",
    "            for i in range(total_timesteps // LOOP_LEN):\n",
    "                model.learn(LOOP_LEN)\n",
    "                model.save(f\"panda_mocap_ppo_model_ckp{i}\")\n",
    "                print(f\"-----------------Save Model: {i}-----------------\")\n",
    "\n",
    "        model.save(\"panda_mocap_ppo_model\")\n",
    "        \n",
    "\n",
    "    # 测试模型\n",
    "    observation, info = env.reset(seed=42)\n",
    "    for test in range(10):\n",
    "        total_reward = 0\n",
    "        for _ in range(1000):\n",
    "            start_time = datetime.now()\n",
    "\n",
    "            action, _states = model.predict(observation, deterministic=True)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # 帧率为 60fps ，为显示为正常速度，每次渲染间隔 16ms\n",
    "            elapsed_time = datetime.now() - start_time\n",
    "            if elapsed_time.total_seconds() < 0.016666666666666666:\n",
    "                await asyncio.sleep(0.016666666666666666 - elapsed_time.total_seconds())\n",
    "\n",
    "            if terminated or truncated:\n",
    "                print(f\"----------------Test: {test}----------------\")\n",
    "                print(\"Terminated: \", terminated, \" Truncated: \", truncated)\n",
    "                print(\"Total Reward: \", total_reward)\n",
    "                print(\"---------------------------------------\")\n",
    "                observation, info = env.reset()\n",
    "                total_reward = 0\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        grpc_address = \"localhost:50051\"\n",
    "        print(\"simulation running... , grpc_address: \", grpc_address)\n",
    "        env_id = f\"PandaMocap-v0-OrcaGym-{grpc_address[-2:]}\"\n",
    "        register_env(grpc_address)\n",
    "\n",
    "        env = gym.make(env_id)\n",
    "        print(\"启动仿真环境\")\n",
    "        asyncio.run(continue_training(env, total_timesteps=200000, is_training=True))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"退出仿真环境\")\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试用代码\n",
    "\n",
    "* 测试控制有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_file_path = os.path.abspath('')\n",
    "project_root = os.path.dirname(current_file_path)\n",
    "\n",
    "# 将项目根目录添加到 PYTHONPATH\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from gymnasium.envs.registration import register\n",
    "from envs.mujoco.franka_emika_panda import FrankaEnv\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "from envs.orca_gym_env import ActionSpaceType\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def register_env(grpc_address, ):\n",
    "    print(\"register_env: \", grpc_address)\n",
    "    gym.register(\n",
    "        id=f\"PandaMocap-v0-OrcaGym-{grpc_address[-2:]}\",\n",
    "        entry_point=\"envs.panda_mocap.reach:FrankaReachEnv\",\n",
    "        kwargs={'frame_skip': 5, \n",
    "                'reward_type': \"dense\",\n",
    "                'action_space_type': ActionSpaceType.CONTINUOUS,\n",
    "                'action_step_count': 0,\n",
    "                'grpc_address': grpc_address, \n",
    "                'agent_names': ['Panda'], \n",
    "                'time_step': 0.01},\n",
    "        max_episode_steps=512,\n",
    "        reward_threshold=0.0,\n",
    "    )\n",
    "\n",
    "async def continue_training(env, total_timesteps, is_training):\n",
    "\n",
    "    # 测试模型\n",
    "    observation, info = env.reset(seed=42)\n",
    "    for test in range(10):\n",
    "        total_reward = 0\n",
    "        for _ in range(1000):\n",
    "            start_time = datetime.now()\n",
    "\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # 帧率为 60fps ，为显示为正常速度，每次渲染间隔 16ms\n",
    "            elapsed_time = datetime.now() - start_time\n",
    "            if elapsed_time.total_seconds() < 0.016:\n",
    "                await asyncio.sleep(0.016 - elapsed_time.total_seconds())\n",
    "\n",
    "            if terminated or truncated:\n",
    "                print(f\"----------------Test: {test}----------------\")\n",
    "                print(\"Terminated: \", terminated, \" Truncated: \", truncated)\n",
    "                print(\"Total Reward: \", total_reward)\n",
    "                print(\"---------------------------------------\")\n",
    "                observation, info = env.reset()\n",
    "                total_reward = 0\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        grpc_address = 'localhost:50051'\n",
    "        print(\"simulation running... , grpc_address: \", grpc_address)\n",
    "        env_id = f\"PandaMocap-v0-OrcaGym-{grpc_address[-2:]}\"\n",
    "        register_env(grpc_address)\n",
    "\n",
    "        env = gym.make(env_id)\n",
    "        print(\"启动仿真环境\")\n",
    "        asyncio.run(continue_training(env, total_timesteps=200000, is_training=True))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"关闭仿真环境\")\n",
    "        env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orca_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
