{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Reinforcement Learning Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify PyTorch Installation\n",
    "* Reinforcement learning training typically requires GPU acceleration, so using the PyTorch framework can significantly improve training performance.\n",
    "* The following code verifies the PyTorch installation and checks the output to determine whether a GPU-accelerated version of PyTorch (compatible with your CUDA version) is installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Current CUDA device:\", torch.cuda.current_device())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError(\"CUDA is not available. Please check your PyTorch installation.\")\n",
    "\n",
    "# Print CUDA device information\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Create a large tensor\n",
    "tensor_size = 20000  # Increase the size of the tensor\n",
    "x = torch.rand((tensor_size, tensor_size), device=device)\n",
    "y = torch.rand((tensor_size, tensor_size), device=device)\n",
    "\n",
    "# Perform multiple matrix multiplication operations and measure the time\n",
    "num_iterations = 10\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    result = torch.mm(x, y)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"{num_iterations} iterations of matrix multiplication completed in {elapsed_time:.4f} seconds.\")\n",
    "print(f\"Average time per iteration: {elapsed_time / num_iterations:.4f} seconds.\")\n",
    "\n",
    "# Check a part of the result to ensure the operation was successful\n",
    "print(\"Result[0, 0]:\", result[0, 0].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Training with 16 Agents\n",
    "\n",
    "1. Open the `Franka_RL` level and click run.\n",
    "2. Execute the code below.\n",
    "3. You will see the Franka robot arms start moving quickly, and the console will print out training information. In this task, the agent's goal is to control the robot arm's end effector to reach the transparent red cube marker. This is a relatively simple task that can achieve a high success rate within 200 episodes for each agent. This will take about 20 minutes to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "script_dir = \"../examples/franka_rl/\"\n",
    "script_path = os.path.join(\"franka_mocap_multi_agents.py\")\n",
    "command = [\n",
    "    \"python\", script_path,\n",
    "    \"--orcagym_addresses\", \"localhost:50051\",\n",
    "    \"--subenv_num\", \"1\",\n",
    "    \"--agent_num\", \"16\",\n",
    "    \"--task\", \"reach\",\n",
    "    \"--model_type\", \"ppo\",\n",
    "    \"--run_mode\", \"training\",\n",
    "    \"--training_episode\", \"200\"\n",
    "]\n",
    "try:\n",
    "    subprocess.run(command, cwd=script_dir)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training stopped by user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. When the training is complete, run the following command to validate the training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "script_dir = \"../examples/franka_rl/\"\n",
    "script_path = os.path.join(\"franka_mocap_multi_agents.py\")\n",
    "command = [\n",
    "    \"python\", script_path,\n",
    "    \"--orcagym_addresses\", \"localhost:50051\",\n",
    "    \"--subenv_num\", \"1\",\n",
    "    \"--agent_num\", \"16\",    \n",
    "    \"--task\", \"reach\",\n",
    "    \"--model_type\", \"ppo\",\n",
    "    \"--run_mode\", \"testing\",\n",
    "    \"--training_episode\", \"200\"\n",
    "]\n",
    "\n",
    "subprocess.run(command, cwd=script_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can try other tasks, such as **pick_and_place**, or explore different training algorithms like **DDPG**. \n",
    "\n",
    "The **pick_and_place** task is significantly more complex than the **reach** task, and you might require more training iterations. To accelerate training, you can use **subenv** to increase concurrency. **Subenv** is a clone of **OrcaGymEnv** that can run without displaying results in **OrcaStudio**. \n",
    "\n",
    "For specific parameters and code examples, refer to **examples/franka_rl/franka_mocap_multi_agents.py**. You can also improve the reward function for the **pick_and_place** or **reach** tasks to achieve better training results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orca_gym_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
